{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# varbvs: A Software Toolkit for Fast Variable Selection in Genome-wide Association Studies and Other Large-scale Regression Applications\n",
    "Peter Carbonetto, Xiang Zhou, Matthew Stephens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Variational Bayesian variable selection (varbvs): \n",
    "1. A software toolkit for the analysis of large-scale data sets using Bayesian variable selection methods; \n",
    "2. Builds on Bayesian models for variable selection in regression and variational approximation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Introduction\n",
    "Bayesian variable selection (BVS) applications:\n",
    "- mapping of complex disease and trait loci\n",
    "- enrichment analysis\n",
    "- estimate the proportion of variance in phenotypes explained by available genotypes\n",
    "- fine-mapping\n",
    "\n",
    "BVS not widely used for GWAS, reasons include:\n",
    "- difficult to use\n",
    "- appropriate specification of priors\n",
    "- efficient computation of posterior probabilities\n",
    "\n",
    "Aims in developing varbvs software:\n",
    "- make BVS methods accessible to practitioners\n",
    "- provide an alternative to commonly used toolkits for penalized sparse regression\n",
    "\n",
    "Advantages of BVS (over penalized sparse regression):\n",
    "- computes the probabilities that each variable is included in the regression model - posterior inclusion probability (PIP)\n",
    "- no cross-validation or false positive rates are required to determine significance levels\n",
    "- allows for the possibility of model comparison through approximate computation of Bayes factors\n",
    "\n",
    "Limitations of BVS:\n",
    "- computing exact posterior probabilities is intractable except in very small data sets\n",
    "- the choice of priors requires considerable expertise in Bayesian data analysis\n",
    "\n",
    "varbvs remedies these two limitations above by:\n",
    "1. implement fast posterior computation using variational approximation techniques\n",
    "2. provide default priors that are suitable, and also allow for extensive prior customization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Example illustrating features of varbvs\n",
    "Introduce an exchangeable prior (spike-and-slab prior) on the coefficients\n",
    "- The complexity of model is controlled by the prior, which is determined by two parameters:\n",
    "    - $\\pi$: the prior log-odds $\\log_{10}(\\frac{\\pi}{1-\\pi})$ that a variable is included in the regression model\n",
    "    - $\\sigma_a^2$: the prior variance of the regression coefficients (sa).\n",
    "\n",
    "\n",
    "-  compute results for different settings of logodds, and keep sa constant to simplify the example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Bayesian variable selection, and the varbvs R interface\n",
    "### Regression model\n",
    "When Y is continuous: assume a basic linear regression by setting `family = \"gaussian\"`\n",
    "\n",
    "$Y=\\sum^m_{i=1}Z_i\\mu_i + \\sum_{i=1}^pX_i\\beta_i+\\epsilon$, where $Z$ is covariates and $\\epsilon \\sim N(0,\\sigma^2)$\n",
    "\n",
    "When Y is binary: an additive model for the log-odds of $Y = 1$ by setting `family = \"binomial\"`\n",
    "\n",
    "$\\log{\\frac{\\Pr(Y=1)}{\\Pr(Y=0)}} = \\sum_{i=1}^mZ_i\\mu_i + \\sum_{i=1}^p X_i\\beta_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Variable selection prior\n",
    "- Adopt \"spike-and-slab\" prior to frame the vriable selection problem\n",
    "    - with probability π, coefficient $β_i$ is drawn from the \"slab\"\n",
    "    - slab distribution: normal density with zero mean and variance $σ^2σ_a^2$\n",
    "    - with probability 1 − π, $β_i$ equals zero, the \"spike\"\n",
    "    - small value of π encourage \"sparse\" regression models, indicates only a small proportion of variable in X help predict Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Hyperparameters in varbvs argument settings\n",
    "Hyperparameters:\n",
    "- `sa`: the prior variance of the regression coefficients\n",
    "- `sigma`: the residual variance for linear regression\n",
    "- `logodds`: the prior log-odds of inclusion, $\\log_{10}\\{\\frac{\\pi}{1-\\pi}\\}$\n",
    "\n",
    "varbvs can encode preferences of hyperparameters\n",
    "- non-exchangeable prior $\\boldsymbol{\\pi}$: set input logodds to a matrix, the rows correspond to variables and the columns correspond to hyperparameter settings.\n",
    "- fit one or more of the hyperparameters to the data: `update.sigma = TRUE` and/or `update.sa = TRUE` in varbvs arguments\n",
    "- if `sigma` and/or `sa` are not provided, varbvs will use the default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Fast posterior computation\n",
    "Computing posterior probabilities is often an intractable, high-dimensional integration problem\n",
    "- sulution in varbvs: introduce a class of approximating distributions, then optimizing a criterion (the Kullback-Leibler divergence) to find the distribution within this class that best matches the posterior.\n",
    "\n",
    "The algorithm for fitting the variational approximation consists of an inner loop and an outer loop\n",
    "- outer loop: iterates over the hyperparameter settings\n",
    "- inner loop: cycles through co-ordinate ascent updates to tighten the lower bound on the marginal likelihood given a setting of the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Averaging over the hyperparameter settings\n",
    "Since different settings are allowed for hyperparameters, they also implement the Bayesian model averaging strategy, in which averaging over settings of the hyperparameters, weighted by the posterior probability of each setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### varbvs function\n",
    "```\n",
    "varbvs(X, Z, y, family,                              # Data.\n",
    "sigma, sa, logodds,                                  # Hyperparameters.\n",
    "alpha, mu, eta,                                      # Variational parameters.\n",
    "update.sigma, update.sa, optimize.eta,               # Optimization and model\n",
    "initialize.params, nr, sa0, n0, tol, maxiter,        # fitting settings.\n",
    "verbose)                                             # Other settings.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Example: mapping Crohn’s disease risk loc\n",
    "Large data set:\n",
    "- 4,686 samples: 1,748 Crohn’s disease cases and 2,938 controls\n",
    "- 442,001 SNPs\n",
    "- binary outcome\n",
    "\n",
    "varbvs results:\n",
    "- the fitted regression model is very sparse: only 8 out of the 442,001 candidate variables are included in the model with probability 0.5 or greater\n",
    "- Top 9 variables by inclusion probability:\n",
    "\n",
    "```\n",
    "  index variable   prob  PVE coef* Pr(coef.>0.95)\n",
    "1 71850 rs10210302 1.000 NA -0.313 [-0.397,-0.236]\n",
    "2 10067 rs11805303 1.000 NA 0.291 [+0.207,+0.377]\n",
    "3 140044 rs17234657 1.000 NA 0.370 [+0.255,+0.484]\n",
    "4 381590 rs17221417 1.000 NA 0.279 [+0.192,+0.371]\n",
    "5 402183 rs2542151 0.992 NA 0.290 [+0.186,+0.392]\n",
    "6 271787 rs10995271 0.987 NA 0.236 [+0.151,+0.323]\n",
    "7 278438 rs7095491 0.969 NA 0.222 [+0.141,+0.303]\n",
    "8 168677 rs9469220 0.586 NA -0.194 [-0.269,-0.118]\n",
    "9 22989 rs12035082 0.485 NA 0.195 [+0.111,+0.277]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Summary and discussion\n",
    "Benefits of BVS\n",
    "- account for uncertainty in hyperparameter\n",
    "- fast computation if all coefficients are conditionally independent\n",
    "\n",
    "Disadvantages\n",
    "- require careful selection of an additional set of priors for the hyperparameters\n",
    "- does not offer complete flexibility: does not include other types of prior, such as g-prior\n",
    "- when variables are completely correlated, each of them are expected to have equal PIP, but varbvs may get this wrong by calculating PIP equal to 1 for only one variable while the other equal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "version": "0.21.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
