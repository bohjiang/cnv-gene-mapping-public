{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Logistic regression spike slab prior using PyMC3\n",
    "\n",
    "Previously we have tried BoomSpikeSlab and LogitBVs R packages to fit spike slab model for logistic regression but didn't get anywhere the way we want (FIXME: add link). Now I'm looking at some customized options. According to [this post](https://www.kaggle.com/melondonkey/bayesian-spike-and-slab-in-pymc3), `stan` cannot handle spike slab model because it is not discrete. The post implements a `PyMC3` based sampler that looks neat enough so I'm trying to use it for our problem here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Indeed discrete prior might also be not optimal for `PyMC3`, as pointed out [in this notebook](https://www.kaggle.com/derekpowll/bayesian-lr-w-cauchy-prior-in-pymc3). I think we can also try a spiky normal plus a slab normal mixture -- at least they will be continous there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Software required\n",
    "\n",
    "```\n",
    "pip install pymc3 -U\n",
    "conda install mkl-service\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [],
   "source": [
    "X_file = 'deletion.genes.block30.for_simu.sample.genes.block_79_137.gz'\n",
    "y_file = 'deletion.genes.block30.for_simu.sample.y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13412,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.loadtxt(y_file, dtype=int)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13412, 59)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.loadtxt(X_file, dtype=float)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "It is 13K samples 59 features for the CNV problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "## Model specification\n",
    "\n",
    "That is, to specify **how the data is generated**. Specifically it is about setting up the spike slab prior for logistic model, $$b\\sim \\pi_0 \\delta_0 + (1-\\pi_0)N(\\mu, \\sigma^2)$$ where from `varbvs` analysis, $\\pi_0 = 0.043, \\mu = 0.77, \\sigma = 0.84$.\n",
    "\n",
    "For intercept since for centered data it has interpretation of baseline odds ratio, I'm giving it a normal prior $N(0, 1.5)$ to roughly cover the span of baseline odds 0.05 (log odds about -3), for a not so rare disease.\n",
    "\n",
    "**Question: how should we handle intercept? How did `varbvs` handle intercept?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import theano.tensor as tt\n",
    "from scipy.special import expit\n",
    "\n",
    "def get_model(y, X, pi0=0.043, mu=0.77, sigma=0.84, mu_intercept=0, sigma_intercept=1.5):\n",
    "    invlogit = lambda x: 1/(1 + tt.exp(-x))\n",
    "    model = pm.Model()\n",
    "    with model:\n",
    "        xi = pm.Bernoulli('xi', pi0, shape=X.shape[1]) #inclusion probability for each variable\n",
    "        alpha = pm.Normal('alpha', mu = mu_intercept, sd = sigma_intercept) # Intercept\n",
    "        beta = pm.Normal('beta', mu = mu, sd = sigma , shape=X.shape[1]) #Prior for the non-zero coefficients\n",
    "        p = pm.math.dot(X, xi * beta) #Deterministic function to map the stochastics to the output\n",
    "        y_obs = pm.Bernoulli('y_obs', invlogit(p + alpha),  observed=y)  #Data likelihood\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [],
   "source": [
    "model = get_model(y,X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "## Sampling\n",
    "\n",
    "Need to read additional `PyMC3` documentation to do proper sampling diagnostics, but generally it helps to use multiple chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (10 chains in 20 jobs)\n",
      "CompoundStep\n",
      ">BinaryGibbsMetropolis: [xi]\n",
      ">NUTS: [beta, alpha]\n",
      "Sampling 10 chains: 100%|██████████| 25000/25000 [43:20<00:00,  6.37draws/s] \n",
      "There were 6 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 2 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 2 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 3 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 3 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 4 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 3 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 4 divergences after tuning. Increase `target_accept` or reparameterize.\n"
     ]
    }
   ],
   "source": [
    "# Here I use 20 cores on my 40 core machine, with 10 chains, to generate 2000 samples.\n",
    "# Takes 50min on my desktop\n",
    "with model:\n",
    "    trace = pm.sample(2000, random_seed = 999, cores = 20, progressbar = True, chains = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "## Results\n",
    "\n",
    "This will summarize samples generated to posterior quantities including PIP, $\\tilde{b}$ and $\\tilde{\\mu}$ ($b$ given inclusion, $\\xi=1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inclusion_probability</th>\n",
       "      <th>beta</th>\n",
       "      <th>beta_given_inclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.03150</td>\n",
       "      <td>0.757005</td>\n",
       "      <td>0.452559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.02910</td>\n",
       "      <td>0.755969</td>\n",
       "      <td>0.331873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.02855</td>\n",
       "      <td>0.764855</td>\n",
       "      <td>0.325449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.02830</td>\n",
       "      <td>0.757791</td>\n",
       "      <td>0.389020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.02820</td>\n",
       "      <td>0.759151</td>\n",
       "      <td>0.502015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.02810</td>\n",
       "      <td>0.757029</td>\n",
       "      <td>0.399929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.02805</td>\n",
       "      <td>0.751557</td>\n",
       "      <td>0.407373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.02655</td>\n",
       "      <td>0.767675</td>\n",
       "      <td>0.411278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.02625</td>\n",
       "      <td>0.758376</td>\n",
       "      <td>0.361729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.02535</td>\n",
       "      <td>0.764275</td>\n",
       "      <td>0.390959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.02525</td>\n",
       "      <td>0.762006</td>\n",
       "      <td>0.386991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.02485</td>\n",
       "      <td>0.762214</td>\n",
       "      <td>0.424749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.02470</td>\n",
       "      <td>0.769649</td>\n",
       "      <td>0.380450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.02400</td>\n",
       "      <td>0.745160</td>\n",
       "      <td>-0.138141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02320</td>\n",
       "      <td>0.753159</td>\n",
       "      <td>0.231699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.02310</td>\n",
       "      <td>0.742319</td>\n",
       "      <td>-0.164724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.02200</td>\n",
       "      <td>0.744523</td>\n",
       "      <td>-0.323563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.02195</td>\n",
       "      <td>0.754094</td>\n",
       "      <td>0.300765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.02085</td>\n",
       "      <td>0.745974</td>\n",
       "      <td>-0.360878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.02070</td>\n",
       "      <td>0.749918</td>\n",
       "      <td>-0.259379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.02000</td>\n",
       "      <td>0.750038</td>\n",
       "      <td>-0.270924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.01935</td>\n",
       "      <td>0.749993</td>\n",
       "      <td>-0.059286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.01925</td>\n",
       "      <td>0.765921</td>\n",
       "      <td>-0.100139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01910</td>\n",
       "      <td>0.747522</td>\n",
       "      <td>-0.069117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.01830</td>\n",
       "      <td>0.757533</td>\n",
       "      <td>-0.205336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.01820</td>\n",
       "      <td>0.753273</td>\n",
       "      <td>-0.054830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.01815</td>\n",
       "      <td>0.753205</td>\n",
       "      <td>-0.077024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.01815</td>\n",
       "      <td>0.748001</td>\n",
       "      <td>-0.103099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01815</td>\n",
       "      <td>0.746751</td>\n",
       "      <td>-0.042893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.01810</td>\n",
       "      <td>0.753236</td>\n",
       "      <td>0.231776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.01800</td>\n",
       "      <td>0.757264</td>\n",
       "      <td>0.260432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.01795</td>\n",
       "      <td>0.748185</td>\n",
       "      <td>-0.169743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.01790</td>\n",
       "      <td>0.753713</td>\n",
       "      <td>-0.170681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.01760</td>\n",
       "      <td>0.755195</td>\n",
       "      <td>-0.077401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.01745</td>\n",
       "      <td>0.756614</td>\n",
       "      <td>-0.236987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01720</td>\n",
       "      <td>0.760598</td>\n",
       "      <td>-0.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.01720</td>\n",
       "      <td>0.761993</td>\n",
       "      <td>0.294846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.01715</td>\n",
       "      <td>0.769173</td>\n",
       "      <td>0.106053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.01675</td>\n",
       "      <td>0.758404</td>\n",
       "      <td>-0.205894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.01675</td>\n",
       "      <td>0.745851</td>\n",
       "      <td>-0.094079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.01645</td>\n",
       "      <td>0.758816</td>\n",
       "      <td>-0.054397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.01645</td>\n",
       "      <td>0.753035</td>\n",
       "      <td>-0.207222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01635</td>\n",
       "      <td>0.760321</td>\n",
       "      <td>-0.270879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.01635</td>\n",
       "      <td>0.746073</td>\n",
       "      <td>-0.251548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.01630</td>\n",
       "      <td>0.760809</td>\n",
       "      <td>0.123611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.01620</td>\n",
       "      <td>0.767337</td>\n",
       "      <td>0.017822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.01590</td>\n",
       "      <td>0.752706</td>\n",
       "      <td>-0.072749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.01460</td>\n",
       "      <td>0.754795</td>\n",
       "      <td>0.090861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.01415</td>\n",
       "      <td>0.756668</td>\n",
       "      <td>0.070202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.01405</td>\n",
       "      <td>0.759994</td>\n",
       "      <td>0.056885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.01375</td>\n",
       "      <td>0.769638</td>\n",
       "      <td>0.203252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.01365</td>\n",
       "      <td>0.757168</td>\n",
       "      <td>0.051544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.01310</td>\n",
       "      <td>0.757963</td>\n",
       "      <td>-0.043344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.01300</td>\n",
       "      <td>0.759784</td>\n",
       "      <td>0.117668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.01220</td>\n",
       "      <td>0.760374</td>\n",
       "      <td>-0.070914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.01205</td>\n",
       "      <td>0.758501</td>\n",
       "      <td>-0.056862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.01185</td>\n",
       "      <td>0.756312</td>\n",
       "      <td>0.131893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.01065</td>\n",
       "      <td>0.759993</td>\n",
       "      <td>0.090245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.01025</td>\n",
       "      <td>0.761029</td>\n",
       "      <td>0.115815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    inclusion_probability      beta  beta_given_inclusion\n",
       "42                0.03150  0.757005              0.452559\n",
       "57                0.02910  0.755969              0.331873\n",
       "58                0.02855  0.764855              0.325449\n",
       "32                0.02830  0.757791              0.389020\n",
       "43                0.02820  0.759151              0.502015\n",
       "23                0.02810  0.757029              0.399929\n",
       "30                0.02805  0.751557              0.407373\n",
       "27                0.02655  0.767675              0.411278\n",
       "25                0.02625  0.758376              0.361729\n",
       "29                0.02535  0.764275              0.390959\n",
       "24                0.02525  0.762006              0.386991\n",
       "26                0.02485  0.762214              0.424749\n",
       "31                0.02470  0.769649              0.380450\n",
       "56                0.02400  0.745160             -0.138141\n",
       "1                 0.02320  0.753159              0.231699\n",
       "55                0.02310  0.742319             -0.164724\n",
       "54                0.02200  0.744523             -0.323563\n",
       "28                0.02195  0.754094              0.300765\n",
       "15                0.02085  0.745974             -0.360878\n",
       "53                0.02070  0.749918             -0.259379\n",
       "52                0.02000  0.750038             -0.270924\n",
       "7                 0.01935  0.749993             -0.059286\n",
       "9                 0.01925  0.765921             -0.100139\n",
       "3                 0.01910  0.747522             -0.069117\n",
       "22                0.01830  0.757533             -0.205336\n",
       "5                 0.01820  0.753273             -0.054830\n",
       "11                0.01815  0.753205             -0.077024\n",
       "10                0.01815  0.748001             -0.103099\n",
       "4                 0.01815  0.746751             -0.042893\n",
       "16                0.01810  0.753236              0.231776\n",
       "17                0.01800  0.757264              0.260432\n",
       "45                0.01795  0.748185             -0.169743\n",
       "46                0.01790  0.753713             -0.170681\n",
       "8                 0.01760  0.755195             -0.077401\n",
       "44                0.01745  0.756614             -0.236987\n",
       "2                 0.01720  0.760598             -0.072100\n",
       "39                0.01720  0.761993              0.294846\n",
       "36                0.01715  0.769173              0.106053\n",
       "13                0.01675  0.758404             -0.205894\n",
       "12                0.01675  0.745851             -0.094079\n",
       "6                 0.01645  0.758816             -0.054397\n",
       "47                0.01645  0.753035             -0.207222\n",
       "0                 0.01635  0.760321             -0.270879\n",
       "14                0.01635  0.746073             -0.251548\n",
       "33                0.01630  0.760809              0.123611\n",
       "21                0.01620  0.767337              0.017822\n",
       "18                0.01590  0.752706             -0.072749\n",
       "34                0.01460  0.754795              0.090861\n",
       "37                0.01415  0.756668              0.070202\n",
       "35                0.01405  0.759994              0.056885\n",
       "19                0.01375  0.769638              0.203252\n",
       "41                0.01365  0.757168              0.051544\n",
       "20                0.01310  0.757963             -0.043344\n",
       "38                0.01300  0.759784              0.117668\n",
       "50                0.01220  0.760374             -0.070914\n",
       "51                0.01205  0.758501             -0.056862\n",
       "40                0.01185  0.756312              0.131893\n",
       "49                0.01065  0.759993              0.090245\n",
       "48                0.01025  0.761029              0.115815"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "results = pd.DataFrame({'inclusion_probability':np.apply_along_axis(np.mean, 0, trace['xi']),\n",
    "                       'beta':np.apply_along_axis(np.mean, 0, trace['beta']),\n",
    "                       'beta_given_inclusion': np.apply_along_axis(np.sum, 0, trace['xi']*trace['beta']) / np.apply_along_axis(np.sum, 0, trace['xi'])\n",
    "                       })\n",
    "results.sort_values('inclusion_probability', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "But the true causal variables are 14 and 31 ... apparently this needs more work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "## Some sanity check\n",
    "\n",
    "1. Does posterior predictive mean $\\tilde{y}$ roughly equal to data mean?\n",
    "2. What's the posterior number of non-zero variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5000008683342422 1.13855\n"
     ]
    }
   ],
   "source": [
    "estimate = trace['beta'] * trace['xi'] \n",
    "y_hat = np.apply_along_axis(np.mean, 1, expit(trace['alpha'] + np.dot(X, np.transpose(estimate) )) )\n",
    "print(np.mean(y_hat), np.sum(results.inclusion_probability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "So the posterior mean suggests 1 variable involved. The prior is $0.043 \\times 50 \\approx 2$ expected. **Need to check this with simulated truth; also should run `varbvs` on this and compare**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Python3",
     "python3",
     "Python3",
     "#FFD91A",
     ""
    ]
   ],
   "version": "0.20.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
