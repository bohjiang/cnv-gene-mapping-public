{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Copy model simulation and analysis workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run 20190717_workflow.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  get_hist\n",
      "  simulate\n",
      "  analyze\n",
      "  default\n",
      "  get_data_hist\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cnv-type deletion\n",
      "  --cwd /home/min/GIT/github/cnv-gene-mapping/data (as path)\n",
      "  --genotype-file  path(f\"{cwd:a}/{cnv_type}.X.gz\")\n",
      "\n",
      "  --phenotype-file  path(f\"{cwd:a}/{cnv_type}.y\") # real CNV data phenotype\n",
      "\n",
      "\n",
      "Sections\n",
      "  get_hist_1, simulate_1, analyze_1:\n",
      "    Workflow Options:\n",
      "      --n-gene-in-block 1 (as int)\n",
      "                        For simulation: get real deletion/duplication CNV data\n",
      "                        and its block n_gene_in_block: get_hist: 1, simulate:\n",
      "                        20~50, analyze: 1\n",
      "  simulate_2:\n",
      "  simulate_3:\n",
      "    Workflow Options:\n",
      "      --sample-size 100000 (as int)\n",
      "      --n-batch 200 (as int)\n",
      "  simulate_4:\n",
      "  simulate_5:\n",
      "    Workflow Options:\n",
      "      --shape 3.0 (as float)\n",
      "                        shape = 3; scale = 1 for gamma shape = 2.191013; scale =\n",
      "                        0.2682398 for normal\n",
      "      --scale 1.0 (as float)\n",
      "      --beta-method normal\n",
      "                        'gamma' or 'normal'\n",
      "      --penetrance 0.05 (as float)\n",
      "      --seed 999999 (as int)\n",
      "      --ctrl-case-ratio 1.0 (as float)\n",
      "      --pi0 0.95 (as float)\n",
      "  analyze_2:\n",
      "    Workflow Options:\n",
      "      --L 1 (as int)\n",
      "      --pve 0.005 (as float)\n",
      "      --method optim\n",
      "      --real TRUE\n",
      "      --simu-pheno VAL (required)\n",
      "  get_hist_2:\n",
      "  default_1, get_data_hist_1:\n",
      "    Workflow Options:\n",
      "      --n-gene-in-block 20 (as int)\n",
      "  default_2:\n",
      "    Workflow Options:\n",
      "      --sample-size 100000 (as int)\n",
      "      --n-batch 200 (as int)\n",
      "  default_3:\n",
      "  default_4:\n",
      "    Workflow Options:\n",
      "      --shape 3 (as int)\n",
      "                        For shape = 3; scale = 1 for gamma shape = 2.191013;\n",
      "                        scale = 0.2682398 for normal\n",
      "      --scale 1 (as int)\n",
      "      --beta-method normal\n",
      "                        'gamma' or 'normal'\n",
      "      --penetrance 0.05 (as float)\n",
      "      --seed 999999 (as int)\n",
      "      --ctrl-case-ratio 1.0 (as float)\n",
      "      --pi0 0.95 (as float)\n",
      "  default_5, get_data_hist_2:\n",
      "  default_6:\n",
      "  default_7:\n",
      "    Workflow Options:\n",
      "      --L 10 (as int)\n",
      "      --pve 0.005 (as float)\n",
      "      --method optim\n"
     ]
    }
   ],
   "source": [
    "!sos run 20190717_workflow.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Run this workflow\n",
    "### Simulation:\n",
    "```\n",
    "sos run dsc/20190717_workflow.ipynb simulate:1-5 --n_gene_in_block 30 --shape 1 --scale 0.5 -s build\n",
    "```\n",
    "$\\pi_0*\\delta + (1-\\pi_0)* \\text{N} (0, 1)$, shape = 0, scale = 1, to test `varbvs`\n",
    "```\n",
    "sos run dsc/20190717_workflow.ipynb simulate:1-5 --n_gene_in_block 30 --shape 0 --scale 1 -s build\n",
    "```\n",
    "### Get histogram\n",
    "- For simulation\n",
    "\n",
    "```\n",
    "sos run dsc/20190717_workflow.ipynb get_hist:1-2 --genotype_file /home/min/GIT/cnv-gene-mapping/data/deletion_simu/deletion.genes.block30.for_simu.sample.X.gz \\\n",
    "--n_gene_in_block 1 -s build\n",
    "```\n",
    "- For real data\n",
    "\n",
    "```\n",
    "sos run dsc/20190717_workflow.ipynb get_hist:1-2 --n_gene_in_block 1 -s build\n",
    "```\n",
    "\n",
    "### Analyze\n",
    "```\n",
    "sos run dsc/20190717_workflow.ipynb susie:1-3 --genotype_file /home/min/GIT/cnv-gene-mapping/data/deletion_simu/deletion.genes.block30.for_simu.sample.X.gz \\\n",
    "--phenotype_file /home/min/GIT/cnv-gene-mapping/data/deletion_simu/deletion.genes.block30.for_simu.sample.y --n_gene_in_block 1 -s build\n",
    "```\n",
    "```\n",
    "sos run dsc/20190717_workflow.ipynb fisher --genotype_file /home/min/GIT/cnv-gene-mapping/data/deletion_simu/deletion.genes.block30.for_simu.sample.X.gz \\\n",
    "--phenotype_file /home/min/GIT/cnv-gene-mapping/data/deletion_simu/deletion.genes.block30.for_simu.sample.y --n_gene_in_block 1 -s build\n",
    "```\n",
    "```\n",
    "sos run dsc/20190717_workflow.ipynb logit --genotype_file /home/min/GIT/cnv-gene-mapping/data/deletion_simu/deletion.genes.block30.for_simu.sample.X.gz \\\n",
    "--phenotype_file /home/min/GIT/cnv-gene-mapping/data/deletion_simu/deletion.genes.block30.for_simu.sample.y --n_gene_in_block 1 -s build\n",
    "```\n",
    "```\n",
    "sos run dsc/20190717_workflow.ipynb pymc3 --genotype_file /home/min/GIT/cnv-gene-mapping/data/deletion_simu/deletion.genes.block30.for_simu.sample.X.gz \\\n",
    "--phenotype_file /home/min/GIT/cnv-gene-mapping/data/deletion_simu/deletion.genes.block30.for_simu.sample.y --n_gene_in_block 1 -s build\n",
    "```\n",
    "### midway\n",
    "```\n",
    "cd /home/gaow/GIT/github/cnv-gene-mapping\n",
    "JOB_OPT=\"-q midway2 -c midway2.yml\"\n",
    "sos run dsc/20190717_workflow.ipynb pymc3 --genotype_file /home/gaow/GIT/github/cnv-gene-mapping/data/deletion_simu/deletion.genes.block30.for_simu.sample.X.gz \\\n",
    "--phenotype_file /home/gaow/GIT/github/cnv-gene-mapping/data/deletion_simu/deletion.genes.block30.for_simu.sample.y --n_gene_in_block 1 -s build \\\n",
    "--job_size 10 $JOB_OPT\n",
    "```\n",
    "`n_gene_in_block` = 20, `sample_size` = 200000\n",
    "```\n",
    "cd /home/gaow/GIT/github/cnv-gene-mapping\n",
    "JOB_OPT=\"-q midway2 -c midway2.yml\"\n",
    "sos run dsc/20190717_workflow.ipynb simulate --n_gene_in_block 20 --shape 0 --scale 1 --sample_size 200000 -s build --job_size 10 $JOB_OPT\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cnv_type = \"deletion\"\n",
    "parameter: cwd = path(\"~/GIT/github/cnv-gene-mapping/data\")\n",
    "parameter: genotype_file = path(f\"{cwd:a}/{cnv_type}.X.gz\")\n",
    "parameter: phenotype_file = path(f\"{cwd:a}/{cnv_type}.y\") # real CNV data phenotype\n",
    "parameter: n_gene_in_block = 1\n",
    "parameter: job_size = 80\n",
    "parameter: mu0 = 0.777072111580423\n",
    "parameter: s0 = 0.8436501699101251\n",
    "parameter: pi0 = 0.0437754961218526\n",
    "def fmtP(x):\n",
    "    return str(x).replace(\".\", \"p\").replace(' ', '_').replace('\"', \"\").replace(\"'\", \"\").replace(\"-\", '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[get_hist_1, simulate_1, susie_1, varbvs_1, fisher_1, pymc3_1, logit_1]\n",
    "# For simulation: get real deletion/duplication CNV data and its block\n",
    "# n_gene_in_block: get_hist: 1, simulate: 20~50, analyze: 1\n",
    "input: genotype_file\n",
    "output: f\"{_input:nn}.genes.block{n_gene_in_block}.gz\", f\"{_input:nn}.block{n_gene_in_block}.forsimu.index.csv\", f\"{_input:nn}.block{n_gene_in_block}.index.csv\"\n",
    "python: expand = '${ }'\n",
    "    import pandas as pd\n",
    "    from operator import itemgetter\n",
    "    from itertools import *\n",
    "    data = pd.read_csv(${_input:r}, compression = \"gzip\", sep = \"\\t\", header = None)\n",
    "    data_clean = data.loc[:, (data != 0).any(axis = 0)]\n",
    "    data_clean.to_csv(${_output[0]:r}, compression = \"gzip\", sep = \"\\t\", header = False, index = False)\n",
    "    indices = list(data_clean.columns)\n",
    "    bound = list()\n",
    "    i = 0; j = 1; n_0 = len(indices)\n",
    "    while (j < n_0):\n",
    "        if indices[j] - indices[i] >= ${n_gene_in_block} and indices[j] - indices[j-1] > 1:\n",
    "            bound.append([indices[i], indices[j-1]])\n",
    "            i = j\n",
    "        j += 1\n",
    "    bound.append([indices[i], indices[j-1]])\n",
    "    bound = [item for item in bound if item[1] != 0]\n",
    "    if bound[-1] == bound[-2]:\n",
    "        bound = bound[:-1]\n",
    "    pd.DataFrame(bound).to_csv(${_output[1]:r}, sep = \"\\t\", header = False, index = False)\n",
    "    span = [item[1] - item[0] for item in bound]\n",
    "    bound2 = list()\n",
    "    start = 0\n",
    "    for i in span:\n",
    "        end = start + i\n",
    "        start = end + 1\n",
    "        bound2.extend([end, start])\n",
    "    bound2 = [0] + bound2[:-1]\n",
    "    bound3 = [bound2[x:x+2] for x in range(0, len(bound2), 2)]\n",
    "    ## bound3: index start from 0\n",
    "    pd.DataFrame(bound3).to_csv(${_output[2]:r}, sep = \"\\t\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fisher_2]\n",
    "output: f\"{_input[0]:n}.fisher.gz\"\n",
    "python: expand = '${ }'\n",
    "    import pandas as pd\n",
    "    ## use stats.fisher_exact instead of \"from fisher import pvalue\", because \"pvalue\" does not generate the constant pvalue, \n",
    "    ## for example, 'pvalue(56,6650,0,6706).two_tail' is not more significant than 'pvalue(24,6682,0,6706).two_tail'\n",
    "    from scipy import stats\n",
    "    data = pd.read_csv(${_input[0]:r}, compression = \"gzip\", sep = \"\\t\", header = None)\n",
    "    y = pd.read_csv(\"${phenotype_file}\", header = None, names = [\"y\"])\n",
    "    xy = pd.concat([y, data], axis = 1, join = 'inner')\n",
    "    xy1 = xy[xy[\"y\"] == 1]\n",
    "    n1 = xy1.shape[0]\n",
    "    xy0 = xy[xy[\"y\"] == 0]\n",
    "    n0 = xy0.shape[0]\n",
    "    res = list()\n",
    "    for i in list(data.columns):\n",
    "        res.append([f\"gene_{i+1}\", sum(xy1.loc[:,i]), n1 - sum(xy1.loc[:,i]), sum(xy0.loc[:,i]), n0 - sum(xy0.loc[:,i]), \n",
    "                    stats.fisher_exact([[sum(xy1.loc[:,i]), sum(xy0.loc[:,i])], [n1 - sum(xy1.loc[:,i]), n0 - sum(xy0.loc[:,i])]])[1]])\n",
    "    pd.DataFrame(res).sort_values(by = 5).to_csv(${_output:r}, compression = \"gzip\", sep = \"\\t\", header = [\"gene\", \"d_c\", \"d_nc\", \"nd_c\", \"nd_nc\", \"p\"], index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "from fisher import pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.672523463727042e-06"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvalue(603,6103,76,6630).two_tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.619337340565899, 1.9104901676695082e-107)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.fisher_exact([[603,76], [6103,6630]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fisher = pd.read_csv(\"/home/min/GIT/cnv-gene-mapping/data/deletion_simu/deletion.genes.block30.for_simu.sample.genes.block1.fisher.gz\", compression = \"gzip\", sep = \"\\t\", header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2285, 6)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fisher.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene</th>\n",
       "      <th>d_c</th>\n",
       "      <th>d_nc</th>\n",
       "      <th>nd_c</th>\n",
       "      <th>nd_nc</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gene_25</td>\n",
       "      <td>603</td>\n",
       "      <td>6103</td>\n",
       "      <td>76</td>\n",
       "      <td>6630</td>\n",
       "      <td>1.910490e-107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gene_28</td>\n",
       "      <td>603</td>\n",
       "      <td>6103</td>\n",
       "      <td>76</td>\n",
       "      <td>6630</td>\n",
       "      <td>1.910490e-107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gene_27</td>\n",
       "      <td>603</td>\n",
       "      <td>6103</td>\n",
       "      <td>76</td>\n",
       "      <td>6630</td>\n",
       "      <td>1.910490e-107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gene_26</td>\n",
       "      <td>603</td>\n",
       "      <td>6103</td>\n",
       "      <td>76</td>\n",
       "      <td>6630</td>\n",
       "      <td>1.910490e-107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gene_29</td>\n",
       "      <td>603</td>\n",
       "      <td>6103</td>\n",
       "      <td>76</td>\n",
       "      <td>6630</td>\n",
       "      <td>1.910490e-107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gene_24</td>\n",
       "      <td>528</td>\n",
       "      <td>6178</td>\n",
       "      <td>61</td>\n",
       "      <td>6645</td>\n",
       "      <td>1.710063e-97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gene_1138</td>\n",
       "      <td>502</td>\n",
       "      <td>6204</td>\n",
       "      <td>79</td>\n",
       "      <td>6627</td>\n",
       "      <td>2.861096e-79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gene_1139</td>\n",
       "      <td>502</td>\n",
       "      <td>6204</td>\n",
       "      <td>79</td>\n",
       "      <td>6627</td>\n",
       "      <td>2.861096e-79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gene_1137</td>\n",
       "      <td>502</td>\n",
       "      <td>6204</td>\n",
       "      <td>79</td>\n",
       "      <td>6627</td>\n",
       "      <td>2.861096e-79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gene_567</td>\n",
       "      <td>132</td>\n",
       "      <td>6574</td>\n",
       "      <td>1</td>\n",
       "      <td>6705</td>\n",
       "      <td>1.296086e-38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        gene  d_c  d_nc  nd_c  nd_nc              p\n",
       "0    gene_25  603  6103    76   6630  1.910490e-107\n",
       "1    gene_28  603  6103    76   6630  1.910490e-107\n",
       "2    gene_27  603  6103    76   6630  1.910490e-107\n",
       "3    gene_26  603  6103    76   6630  1.910490e-107\n",
       "4    gene_29  603  6103    76   6630  1.910490e-107\n",
       "5    gene_24  528  6178    61   6645   1.710063e-97\n",
       "6  gene_1138  502  6204    79   6627   2.861096e-79\n",
       "7  gene_1139  502  6204    79   6627   2.861096e-79\n",
       "8  gene_1137  502  6204    79   6627   2.861096e-79\n",
       "9   gene_567  132  6574     1   6705   1.296086e-38"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fisher[fisher[\"p\"] < 0.05].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[susie_2, varbvs_2, pymc3_2, logit_2]\n",
    "## R: fread(${_input:[0]}, select = ${_blocks.replace('_', ':')})\n",
    "## similar to fine mapping, create 527 folders and save results for each of them\n",
    "blocks = ['_'.join(x.strip().split()) for x in open(f'{_input[2]:a}').readlines()]#[:5]\n",
    "input: for_each = ['blocks']\n",
    "output: f\"{_input[0]:d}/block_{_blocks}/{_input[0]:bnn}.block_{_blocks}.gz\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '10m', mem = '8G', cores = 1, tags = f'pymc3_{_output:bn}'\n",
    "python: expand = '${ }'\n",
    "    import pandas as pd\n",
    "    data = pd.read_csv(${_input[0]:r}, compression = \"gzip\", sep = \"\\t\", header = None)\n",
    "    data.loc[:, int('${_blocks}'.split(\"_\")[0]):int('${_blocks}'.split(\"_\")[1])].to_csv(${_output:r}, compression = \"gzip\", sep = \"\\t\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[pymc3_3]\n",
    "parameter: iteration = 2000\n",
    "parameter: pymc3_seed = 999\n",
    "parameter: intercept = -2.9444389791664403\n",
    "parameter: sigma_intercept = 0.05\n",
    "input: group_by = 1\n",
    "output: f\"{_input[0]:n}.pymc3.gz\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '10m', mem = '8G', cores = 1, tags = f'pymc3_{_output:bn}'\n",
    "python: expand = '${ }'\n",
    "    import numpy as np, pandas as pd, pymc3 as pm, theano.tensor as tt\n",
    "    from scipy.special import expit\n",
    "    X = pd.read_csv(${_input:r}, compression = \"gzip\", sep = \"\\t\", header = None, dtype = float)\n",
    "    y = np.loadtxt(\"${phenotype_file}\", dtype = int)\n",
    "    invlogit = lambda x: 1/(1 + tt.exp(-x))\n",
    "    model = pm.Model()\n",
    "    with model:\n",
    "        xi = pm.Bernoulli('xi', ${pi0}, shape = X.shape[1]) #inclusion probability for each variable\n",
    "        alpha = pm.Normal('alpha', mu = ${intercept}, sd = ${sigma_intercept}) # Intercept\n",
    "        beta = pm.Normal('beta', mu = ${mu0}, sd = ${s0}, shape = X.shape[1]) #Prior for the non-zero coefficients\n",
    "        p = pm.math.dot(X, xi * beta) #Deterministic function to map the stochastics to the output\n",
    "        y_obs = pm.Bernoulli('y_obs', invlogit(p + alpha), observed = y)  #Data likelihood\n",
    "    with model:\n",
    "        trace = pm.sample(${iteration}, random_seed = ${pymc3_seed}, cores = 1, progressbar = False, chains = 1)\n",
    "    results = pd.DataFrame({'inclusion_probability': np.apply_along_axis(np.mean, 0, trace['xi']),\n",
    "                            'beta': np.apply_along_axis(np.mean, 0, np.multiply(trace[\"beta\"], trace[\"xi\"])),\n",
    "                            'beta_given_inclusion': np.apply_along_axis(np.sum, 0, trace['xi'] * trace['beta']) / np.apply_along_axis(np.sum, 0, trace['xi'])\n",
    "                            })\n",
    "    results.to_csv(${_output:r}, compression = \"gzip\", sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[pymc3_4]\n",
    "input: group_by = \"all\"\n",
    "output: f\"{_input[0]:dd}/{_input[0]:bnnn}.pymc3.all.blocks.pip.gz\"\n",
    "python: expand = '${ }'\n",
    "    import pandas as pd\n",
    "    files = list([${_input:r,}])\n",
    "    res = pd.DataFrame(columns = [\"inclusion_probability\"])\n",
    "    for f in files:\n",
    "        tmp = pd.read_csv(f, compression = \"gzip\", sep = \"\\t\", header = 0, usecols = [0])\n",
    "        res = pd.concat([res, tmp])\n",
    "    res.to_csv(${_output:r}, compression = \"gzip\", sep = \"\\t\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[logit_3]\n",
    "depends: R_library(\"data.table\")\n",
    "input: group_by = 1\n",
    "output: f\"{_input[0]:n}.logit.rds\"\n",
    "R: expand = '${ }', stderr = f'{_input[0]:n}.logit.stderr', stdout = f'{_input[0]:n}.logit.stdout'\n",
    "    source(\"${cwd:dd}/logistic/code/misc.R\")  \n",
    "    source(\"${cwd:dd}/logistic/code/bayes.R\")\n",
    "    X <- as.matrix(data.table::fread(${_input:r}, header = F))\n",
    "    X <- scale(X, center = TRUE, scale = FALSE)\n",
    "    y <- as.matrix(data.table::fread(\"${phenotype_file}\"))\n",
    "    p <- dim(X)[2]\n",
    "    p0 <- rep(${pi0}, 1, p)\n",
    "    out <- bayes.logistic(X, y, p0, ${mu0}, ${s0})\n",
    "    saveRDS(out, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[logit_4]\n",
    "input: group_by = \"all\"\n",
    "output: f\"{_input[0]:dd}/{_input[0]:bnnn}.logit.all.blocks.pip.csv\"\n",
    "R: expand = '${ }', stderr = f'{_input[0]:n}.logit.stderr', stdout = f'{_input[0]:n}.logit.stdout'\n",
    "    files = c(${_input:r,})\n",
    "    pips = c()\n",
    "    for (i in 1:length(files)){pips = c(pips, readRDS(files[i])[[\"p1\"]])}\n",
    "    write(pips, file = ${_output[0]:r}, ncolumns = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[susie_3]\n",
    "depends: R_library(\"data.table\"), R_library('susieR'), R_library(\"reticulate\")\n",
    "parameter: L = 1\n",
    "parameter: pve = 0.005\n",
    "parameter: method = \"optim\"\n",
    "suffix = f\"SuSiE.L_{L}.prior_{fmtP(pve)}\"\n",
    "input: group_by = 1\n",
    "output: f\"{_input[0]:n}.{suffix}.susie.rds\"\n",
    "R: expand = '${ }', stderr = f'{_input[0]:n}.susie.stderr', stdout = f'{_input[0]:n}.susie.stdout'\n",
    "    library(susieR)\n",
    "    library(data.table)\n",
    "    library(reticulate)\n",
    "    X <- as.matrix(data.table::fread(${_input:r}))\n",
    "    y <- as.matrix(data.table::fread(\"${phenotype_file}\"))\n",
    "    storage.mode(X) = 'double'\n",
    "    storage.mode(y) = 'double'\n",
    "    res <- susie(X, y, L = ${L}, scaled_prior_variance = ${pve}, estimate_prior_method = '${method}', estimate_prior_variance = TRUE)\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[susie_4]\n",
    "input: group_by = \"all\"\n",
    "output: f\"{_input[0]:dd}/{_input[0]:bnnn}.susie.all.blocks.pip.csv\", f\"{_input[0]:dd}/{_input[0]:bnnn}.susie.all.blocks.pip.sum.csv\"\n",
    "R: expand = '${ }', stderr = f'{_input[0]:n}.susie.stderr', stdout = f'{_input[0]:n}.susie.stdout'\n",
    "    files = c(${_input:r,})\n",
    "    pips = c()\n",
    "    for (i in 1:length(files)){pips = c(pips, readRDS(files[i])$pip)}\n",
    "    write(pips, file = ${_output[0]:r}, ncolumns = 1)\n",
    "    count = sapply(1:length(files), function(i) sum(readRDS(files[i])$pip))\n",
    "    write(count, file = ${_output[1]:r}, ncolumns = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[varbvs_3]\n",
    "depends: R_library(\"data.table\"), R_library(\"reticulate\"), R_library(\"varbvs\")\n",
    "output: f\"{_input:n}.varbvs.rds\"\n",
    "R: expand = '${ }', stderr = f'{_input[0]:n}.varbvs.stderr', stdout = f'{_input[0]:n}.varbvs.stdout'\n",
    "    library(varbvs)\n",
    "    library(data.table)\n",
    "    X <- as.matrix(data.table::fread(${_input:r}))\n",
    "    y <- as.matrix(data.table::fread(\"${phenotype_file}\"))\n",
    "    storage.mode(X) = 'double'\n",
    "    storage.mode(y) = 'double'\n",
    "    # logodds <- seq(-log10(ncol(X)), 0, length.out = 20)\n",
    "    fit <- varbvs::varbvs(X, NULL, y, family = \"binomial\", update.b0 = TRUE, verbose = FALSE)\n",
    "    saveRDS(fit, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[varbvs_4]\n",
    "input: group_by = \"all\"\n",
    "output: f\"{_input[0]:dd}/{_input[0]:bnnn}.varbvs.all.blocks.pip.csv\", f\"{_input[0]:dd}/{_input[0]:bnnn}.varbvs.all.blocks.pip.sum.csv\"\n",
    "R: expand = '${ }', stderr = f'{_input[0]:n}.varbvs.stderr', stdout = f'{_input[0]:n}.varbvs.stdout'\n",
    "    files = c(${_input:r,})\n",
    "    pips = c()\n",
    "    for (i in 1:length(files)){pips = c(pips, readRDS(files[i])$pip)}\n",
    "    write(pips, file = ${_output[0]:r}, ncolumns = 1)\n",
    "    count = sapply(1:length(files), function(i) sum(readRDS(files[i])$pip))\n",
    "    write(count, file = ${_output[1]:r}, ncolumns = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[get_hist_2]\n",
    "output: f\"{_input[0]:n}.histogram.pdf\"\n",
    "python: expand = '${ }'\n",
    "    import pandas as pd, matplotlib.pyplot as plt\n",
    "    blocks = pd.read_csv(${_input[1]:r}, sep = \"\\t\", header = None, names = [\"start\", \"end\"])\n",
    "    spans = [j-i+1 for i,j in zip(blocks[\"start\"], blocks[\"end\"])]\n",
    "    counts = {i: spans.count(i) for i in set(spans) if i != 0}\n",
    "    fig, ax = plt.subplots(figsize = (8,6))\n",
    "    plt.bar(list(counts.keys()), list(counts.values()), width = 0.8)\n",
    "    ax.set_title(\"Histogram of number of genes in blocks\")\n",
    "    plt.savefig(${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[simulate_2]\n",
    "output: f\"{_input[0]:n}.for_simu.gz\"\n",
    "python: expand = '${ }'\n",
    "    import pandas as pd\n",
    "    data = pd.read_csv(${genotype_file:r}, compression = \"gzip\", header = None, sep = \"\\t\")\n",
    "    bound = pd.read_csv(${_input[1]:r}, header = None, sep = \"\\t\")\n",
    "    bound2 = [[item[0], item[1]] if item[0] == bound.values[-1][0] else [item[0], bound.values[j+1][0]-1] for j, item in enumerate(bound.values)]\n",
    "    fill = list()\n",
    "    for l in range(data.shape[0]):\n",
    "        fill.append([data.loc[l, k[0]:k[1]].tolist() for k in bound2])\n",
    "    res = pd.DataFrame(fill)\n",
    "    res.to_csv(${_output:r}, compression = \"gzip\", sep = \"\\t\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The output of this step has two files:\n",
    "- Boundary file\n",
    "\n",
    "        block_start     block_end\n",
    "        14      23\n",
    "        93      97\n",
    "        164     177\n",
    "        229     236\n",
    "- Genotype matrix with column names from boundary: column names are index from start to end in each boundary.\n",
    "\n",
    "        14      15      16      17      18      19      20      21      22      23      93      94    95       96      97\n",
    "         0       1       1       0       1       1       1       1       0       0       0       0     1        1       1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[simulate_3]\n",
    "parameter: sample_size = 100000 # sample size: default 100000, test: 1000\n",
    "parameter: n_batch = 200 # number of simulated sample for each job, default: 200, test: 20\n",
    "assert sample_size % n_batch == 0\n",
    "batches = [x+1 for x in range(n_batch)]\n",
    "input: for_each = ['batches']\n",
    "output: f\"{cwd:a}/{cnv_type}_simu_{n_gene_in_block}/{_input[0]:bn}.sample.{_batches}.gz\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '10m', mem = '8G', cores = 1, tags = f'simulate3_{_output:bn}'\n",
    "python: expand = '${ }'\n",
    "    import pandas as pd, numpy as np\n",
    "    import random, itertools, ast\n",
    "    data = pd.read_csv(${_input:r}, compression = \"gzip\", header = None, sep = \"\\t\")\n",
    "    size = int(${sample_size} / ${n_batch})\n",
    "    random.seed(${_batches})\n",
    "    samples_genome = list()\n",
    "    for i in range(size):\n",
    "        order = random.sample(data.index.tolist(), data.shape[1])\n",
    "        s = list(itertools.chain(*list(ast.literal_eval(n) for n in np.diag(data.loc[order, :]))))\n",
    "        samples_genome.append(s)\n",
    "    samples_genome_df = pd.DataFrame(samples_genome) # row: sample, column: gene\n",
    "    samples_genome_df.to_csv(${_output:r}, compression = \"gzip\", sep = \"\\t\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[simulate_4]\n",
    "input: group_by = 'all'\n",
    "output: f'{_input[0]:nn}.combined.gz'\n",
    "bash: expand = \"${ }\"\n",
    "    zcat ${_input} | gzip > ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[simulate_5]\n",
    "# shape = 3; scale = 1 for gamma\n",
    "# shape = 2.191013; scale = 0.2682398 for normal\n",
    "parameter: shape = 3.0 # mean for normal (1), shape for gamma (3)\n",
    "parameter: scale = 1.0 # se for normal (0.5), scale for gamma (1)\n",
    "# 'gamma' or 'normal'\n",
    "parameter: beta_method = 'normal'\n",
    "parameter: penetrance = 0.05\n",
    "parameter: seed = 999999\n",
    "parameter: ctrl_case_ratio = 1.0\n",
    "parameter: pi0 = 0.95\n",
    "output: f'{_input:nn}.shape{shape}.scale{scale}.X.gz', f'{_input:nn}.shape{shape}.scale{scale}.y', f'{_input:nn}.shape{shape}.scale{scale}.beta'\n",
    "python: expand = \"${ }\"\n",
    "    import pandas as pd, numpy as np\n",
    "    np.random.seed(${seed})\n",
    "    # For normal distribution the -3*sigma to 3*sigma on x-axis should correspond to\n",
    "    # log(4) and log(20). The shape and scale parameters are thus:\n",
    "    # mu = (log(20) + log(4))/2 = 2.191013; sigma = (log(20) - mu) / 3 = 0.2682398\n",
    "    def logor_gamma(shape, scale, n):\n",
    "        return np.log(np.random.gamma(shape, scale, n))\n",
    "\n",
    "    def logor_normal(mean, se, n):\n",
    "        return np.random.normal(mean, se, n)\n",
    "\n",
    "    data = pd.read_csv(${_input:r}, compression = \"gzip\", sep = \"\\t\", header = None)\n",
    "    beta0 = np.log(${penetrance} / (1-${penetrance}))\n",
    "    beta1s = [x for x in logor_${beta_method}(${shape}, ${scale}, data.shape[1])]\n",
    "    beta1s = [np.random.binomial(1, 1-${pi0}) * i for i in beta1s]\n",
    "    ## FIXME: store sparse beta: non-zero beta's with their indices\n",
    "    with open(${_output[2]:r}, 'w') as f:\n",
    "        f.write(\"\\n\".join([str(b) for b in beta1s]))\n",
    "    logit_y = np.matmul(data.values, beta1s) + beta0\n",
    "    ys_p = np.exp(logit_y) / (1+np.exp(logit_y))\n",
    "    ys = np.random.binomial(1, ys_p)\n",
    "    case_index = np.ravel(np.where(ys == 1))\n",
    "    ctrl_index = sorted(np.random.choice(np.ravel(np.where(ys == 0)), int(len(case_index) * ${ctrl_case_ratio})))\n",
    "    genotype = data.iloc[case_index.tolist() + ctrl_index, :]\n",
    "    genotype.to_csv(${_output[0]:r}, compression = \"gzip\", sep = \"\\t\", header = False, index = False)\n",
    "    with open(${_output[1]:r}, 'w') as f:\n",
    "        f.write('\\n'.join(['1'] * len(case_index) + ['0'] * len(ctrl_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Note\n",
    "```\n",
    "cd ~/GIT/cnv-gene-mapping\n",
    "sos run dsc/20190717_workflow.ipynb get_hist:1-2 -s build\n",
    "sos run dsc/20190717_workflow.ipynb get_hist:1-2 --genotype_file /home/min/GIT/cnv-gene-mapping/data/deletion_simu/deletion.genes.block30.for_simu.sample.X.gz\n",
    "sos run dsc/20190717_workflow.ipynb analyze:1-2 -s build\n",
    "sos run dsc/20190717_workflow.ipynb analyze:1-2 --simu_pheno /home/min/GIT/cnv-gene-mapping/data/deletion_simu/deletion.genes.block30.for_simu.sample.y -s build\n",
    "sos run dsc/20190717_workflow.ipynb simulate:1-5 --n_gene_in_block 30 --shape 1 --scale 0.5 -s build\n",
    "sos run dsc/20190717_workflow.ipynb -s build -j 6\n",
    "```\n",
    "```\n",
    "sinteractive --time=01:00:00 --partition=bigmem2 --nodes=1 --ntasks-per-node=1 --mem-per-cpu=100G\n",
    "sos run dsc/20190717_workflow.ipynb simulate:1-5 --n_gene_in_block 30 --shape 1 --scale 0.5 -s build\n",
    "\n",
    "sos run dsc/20190717_workflow.ipynb get_hist:1-2 --genotype_file /home/gaow/GIT/github/cnv-gene-mapping/data/deletion_simu/deletion.genes.block30.for_simu.sample.X.gz \\\n",
    "--phenotype_file /home/gaow/GIT/github/cnv-gene-mapping/data/deletion_simu/deletion.genes.block30.for_simu.sample.y --n_gene_in_block 1 -s build\n",
    "\n",
    "sos run dsc/20190717_workflow.ipynb analyze:1-2 --genotype_file /home/gaow/GIT/github/cnv-gene-mapping/data/deletion_simu/deletion.genes.block30.for_simu.sample.X.gz \\\n",
    "--phenotype_file /home/gaow/GIT/github/cnv-gene-mapping/data/deletion_simu/deletion.genes.block30.for_simu.sample.y --n_gene_in_block 1 \\\n",
    "--simu_pheno /home/gaow/GIT/github/cnv-gene-mapping/data/deletion_simu/deletion.genes.block30.for_simu.sample.y --real \"FALSE\" -s build\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     "shell"
    ],
    [
     "R",
     "ir",
     "R",
     "#DCDCDA",
     "r"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.20.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
